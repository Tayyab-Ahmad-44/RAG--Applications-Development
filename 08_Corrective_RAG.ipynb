{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBlEBnTnm_w9",
        "outputId": "53cc0e3f-83ab-46bf-bd49-e322ac7e79c3"
      },
      "outputs": [],
      "source": [
        "# pip install langgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Corrective RAG App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Video Available on LinkedIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "qamuH_mZm6Bx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from langchain_community.document_loaders import FireCrawlLoader\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
        "from langchain.docstore.document import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aefGtoHWm6CR",
        "outputId": "7d5d2ee6-4987-4db6-df84-2cbc0f37a985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "397\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "urls = [\n",
        "    \"https://en.wikipedia.org/wiki/Computer_programming\",\n",
        "    \"https://en.wikipedia.org/wiki/List_of_programming_languages\"\n",
        "]\n",
        "\n",
        "docs = [FireCrawlLoader(url = url, mode = \"scrape\").load() for url in urls]\n",
        "\n",
        "# Split Documents\n",
        "\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size = 250, chunk_overlap = 20\n",
        ")\n",
        "\n",
        "docs_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "print(len(docs_splits))\n",
        "\n",
        "# Filter out complex metadata and ensure proper document formatting\n",
        "filtered_docs = []\n",
        "for doc in docs_splits:\n",
        "    # Ensure that doc is an instance of Document and has a metadata attribute\n",
        "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
        "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
        "        filtered_docs.append(Document(page_content=doc.page_content, metadata = clean_metadata))\n",
        "\n",
        "len(filtered_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj9kV6YJm6Ca"
      },
      "source": [
        "### Add to Vectordb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "Zn8ipi48m6Cp"
      },
      "outputs": [],
      "source": [
        "vector_store = Chroma.from_documents(\n",
        "    documents = filtered_docs,\n",
        "    collection_name = \"rag-chroma\",\n",
        "    embedding = GPT4AllEmbeddings(),\n",
        ")\n",
        "\n",
        "retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwb3qaQus7hq"
      },
      "source": [
        "## Grade Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "JjMdO127m6Cr"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_core.output_parsers import JsonOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZlFJOl9u_I2"
      },
      "source": [
        "### LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "Qobch2D-tZIA"
      },
      "outputs": [],
      "source": [
        "llm = ChatGroq(temperature = 0, model = \"llama3-70b-8192\",)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GapDF3s-32i8"
      },
      "source": [
        "\n",
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "7WsvMd7Jt7VG"
      },
      "outputs": [],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance\n",
        "    of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
        "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    In Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
        "    Provide the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
        "      <|eot_id|><|start_header_id|>user<|end header_id|>\n",
        "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
        "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\",\n",
        "    input_variables = [\"question\", \"document\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRC45w7A4C5z"
      },
      "source": [
        "### Relevence Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nzpSyT6uqEA",
        "outputId": "3fa2bee8-811f-4937-b9d8-0658714f7d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'score': 'no'}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "retrieval_grader = prompt | llm | JsonOutputParser()\n",
        "question = \"C++\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZXAGEHt4YzX"
      },
      "source": [
        "## Generate Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "iNEdELpy4Avd"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkfgUWtL4-yb"
      },
      "source": [
        "### Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "y04Uoce44-NV"
      },
      "outputs": [],
      "source": [
        "\n",
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>systenm<|end_header_id|> You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrien context to answer the question. If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Question: {question}\n",
        "    Context: {context}\n",
        "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"question\", \"document\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ4NJIAeLSpH"
      },
      "source": [
        "### Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "FgYVFNdf6fvt"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO78v6JqO1nA"
      },
      "source": [
        "### Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "aCQNt2zhLfsf"
      },
      "outputs": [],
      "source": [
        "rag_chain = prompt | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uhrxw1J0O5pC"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnG23TrBO3sz",
        "outputId": "88dc4e6c-26aa-4bef-a6ba-b5fad401b646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To program, you need to design and implement algorithms, which are step-by-step specifications of procedures, by writing code in one or more programming languages. Programmers typically use high-level programming languages that are more easily intelligible to humans than machine code.\n"
          ]
        }
      ],
      "source": [
        "question = \"So, how to program.\"\n",
        "docs = retriever.invoke(question)\n",
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOUWKnbWSnCZ"
      },
      "source": [
        "\n",
        "## Web Search via Tavily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "K6NrJKinSt7D"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "7GElC14BTll8"
      },
      "outputs": [],
      "source": [
        "web_search_tool = TavilySearchResults(k = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA63jo4cce6z"
      },
      "source": [
        "## Hallucination Checker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVLVKipEcRpW",
        "outputId": "f44c5cbb-25c7-47c5-e203-e4858fff47f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 'yes'}"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = \"\"\" </begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether\n",
        "    an answer is grounded in / supported by a set of facts. Give a binary score 'yes' or 'no' score to indicate\n",
        "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a\n",
        "    single key 'score and no preamble or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "    Here are the facts:\n",
        "    \\n ------- \\n\n",
        "    {documents}\n",
        "    \\n ------- \\n\n",
        "    Here is the answer: {generation} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"generation\", \"documents\"],\n",
        ")\n",
        "\n",
        "hallucination_grader = prompt | llm | JsonOutputParser()\n",
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht5-DKJ9cSKz"
      },
      "source": [
        "## Answer Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFiehY37TuYx",
        "outputId": "69a4e516-fa77-4b57-a15a-f80e1bfaa322"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 'yes'}"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = PromptTemplate(\n",
        "    template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an\n",
        "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is\n",
        "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
        "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
        "    \\n ------- \\n\n",
        "    {generation}\n",
        "    \\n ------- \\n\n",
        "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
        "    input_variables=[\"generation\", \"question\"],\n",
        ")\n",
        "\n",
        "answer_grader = prompt | llm | JsonOutputParser()\n",
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4Wz856IieVy"
      },
      "source": [
        "## Lang Graph - Setup states and nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "QF0T5XcpicPq"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwS71nJVjdM0"
      },
      "source": [
        "### State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "7bGFZ7XSirCr"
      },
      "outputs": [],
      "source": [
        "class GraphState(TypedDict):\n",
        "  \"\"\"\n",
        "  Represents the state of out graph\n",
        "\n",
        "  Attributes:\n",
        "    question - your question\n",
        "    generation - llm generation\n",
        "    web_search - whether to add search\n",
        "    documents - list of documents\n",
        "  \"\"\"\n",
        "\n",
        "  question : str\n",
        "  generation : str\n",
        "  web_search : str\n",
        "  documents : List[str]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWdWmUyhlo_N"
      },
      "source": [
        "### Nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbrmh6LkwQbw"
      },
      "source": [
        "Retrieve Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "55TeaeTtloqZ"
      },
      "outputs": [],
      "source": [
        "def retrieve(state):\n",
        "  \"\"\"\n",
        "  Retrieve docs from vector store\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): New key added to state, documents, that contains retrieved documents.\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---RETRIEVE---\")\n",
        "  question = state[\"question\"]\n",
        "\n",
        "  documents = retriever.invoke(question)\n",
        "  return {\"documents\": documents, \"question\": question}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PlrsOfmwYS4"
      },
      "source": [
        "Grade Document Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "qRgMf7T2wW9P"
      },
      "outputs": [],
      "source": [
        "\n",
        "def grade_documents(state):\n",
        "  \"\"\"\n",
        "  Determines whether the retrieved documents are relevant to the question\n",
        "  If any document is not relevant, we will set a flag to run web search\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): Filtered out irrelevant documents and updated web-search state\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  # Score each doc\n",
        "  filtered_docs = []\n",
        "  web_search = \"No\"\n",
        "  for d in documents:\n",
        "    score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
        "    grade = score[\"score\"]\n",
        "\n",
        "    # Doc is relevant\n",
        "    if grade.lower() == \"yes\":\n",
        "      print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "      filtered_docs.append(d)\n",
        "    # Doc is irrelevant\n",
        "    else:\n",
        "      print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "      # We dont include this in filtered_docs\n",
        "      # We set a flag to indicate that whether we want to run web search\n",
        "\n",
        "      web_search = \"Yes\"\n",
        "      continue\n",
        "\n",
        "  return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIpq7TPbwcoa"
      },
      "source": [
        "Generate Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "9zRMEVenwe58"
      },
      "outputs": [],
      "source": [
        "def generate(state):\n",
        "  \"\"\"\n",
        "  Generate answer using RAG on retrieved documents\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): New key added to state, generation, that contains LLM generation\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---GENERATE---\")\n",
        "\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "\n",
        "  return {\"documents\": documents, \"question\": question, \"generation\": generation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxpp6GgszKcw"
      },
      "source": [
        "Web Search Node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "m1Bp6GR7zF6J"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def web_search(state):\n",
        "  \"\"\"\n",
        "  Web Search based on the question\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    state (dict): Appended web results to documents\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---Web Search---\")\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "\n",
        "  docs = web_search_tool.invoke({\"query\": question})\n",
        "  web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "  web_results = Document(page_content=web_results)\n",
        "\n",
        "  if documents is not None:\n",
        "    documents.append(web_results)\n",
        "  else:\n",
        "    documents = [web_results]\n",
        "\n",
        "  return {\"question\": question, \"documents\": documents}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0HtH6vw3H48"
      },
      "source": [
        "Conditional Edge: Relevant document or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "Ij48-uXl2Ng7"
      },
      "outputs": [],
      "source": [
        "def decide_to_generate(state):\n",
        "  \"\"\"\n",
        "  Decides whether to generate an answer or add web_search\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    str: Binary decision for next node to call\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "\n",
        "  question = state[\"question\"]\n",
        "  web_search = state[\"web_search\"] # \"yes\" or \"no\"\n",
        "  filtered_documents = state[\"documents\"]\n",
        "\n",
        "  if web_search == \"Yes\":\n",
        "    print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\")\n",
        "    return \"websearch\"\n",
        "\n",
        "  else:\n",
        "    print(\"---DECISION: GENERATE---\")\n",
        "    return \"generate\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc6yigPB69EV"
      },
      "source": [
        "Conditional Edge: Answer is Hallucinating or Not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "_2Nk6XXp673R"
      },
      "outputs": [],
      "source": [
        "def check_hallucination(state):\n",
        "  \"\"\"\n",
        "  Decides whether to generate answer again or not\n",
        "\n",
        "  Args:\n",
        "    state (dict): The current graph state\n",
        "\n",
        "  Returns:\n",
        "    str: Binary decision for next node to call\n",
        "  \"\"\"\n",
        "\n",
        "  question = state[\"question\"]\n",
        "  documents = state[\"documents\"]\n",
        "  generation = state[\"generation\"]\n",
        "\n",
        "  score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
        "  grade = score[\"score\"]\n",
        "\n",
        "  # Check Hallucination\n",
        "  if grade == \"yes\":\n",
        "    print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "    # Check question-answering\n",
        "    print(\"---GRADE GENERATION VS QUESTION---\")\n",
        "    score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "    grade = score[\"score\"]\n",
        "    if grade == \"yes\":\n",
        "      print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "      return \"useful\"\n",
        "    else:\n",
        "      print(\"---DECISION: GENERATION DOES NOT ADDRESSES QUESTION---\")\n",
        "      return \"not useful\"\n",
        "\n",
        "  else:\n",
        "    print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "    return \"not supported\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84wiTjDyGS-h"
      },
      "source": [
        "## Connect Nodes and Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "1u7vfmgu_IrK"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(GraphState)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n8UpydXH_14"
      },
      "source": [
        "### Define the nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "a4PMzhyaGnkv"
      },
      "outputs": [],
      "source": [
        "workflow.add_node(\"websearch\", web_search)\n",
        "workflow.add_node(\"retrieve\", retrieve)\n",
        "workflow.add_node(\"grade_documents\", grade_documents)\n",
        "workflow.add_node(\"generate\", generate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvRwV1ANIc_K"
      },
      "source": [
        "### Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "EkI4YAfrIZiW"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    {\n",
        "        \"websearch\":\"websearch\",\n",
        "        \"generate\":\"generate\"\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"websearch\", \"generate\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    check_hallucination,\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"websearch\"\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KErZoRW1T54b"
      },
      "source": [
        "## Compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "SOSTHa1JT5SY"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fHMUmjpWXTp"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ID19G5IEQ-5s",
        "outputId": "69865cc6-b2a9-4d7d-dcd5-25e0d0f8c5c3"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "inputs = {\"question\": \"How many hours of study needed to master programming.\"}\n",
        "\n",
        "for output in app.stream(inputs):\n",
        "  for key, value in output.items():\n",
        "    pprint(f\"Finished Running: {key}:\")\n",
        "\n",
        "print()\n",
        "print(value[\"generation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLhZc17jCNq5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
