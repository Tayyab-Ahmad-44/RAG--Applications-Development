{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan and Execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"AGENTOPS_API_KEY\"] = os.getenv(\"AGENTOPS_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: \u001b[34m\u001b[34mSession Replay: https://app.agentops.ai/drilldown?session_id=c67902fb-10d6-41c9-a2dc-28776d7df80d\u001b[0m\u001b[0m\n",
      "WARNING:root:ðŸš¨Importing the Langchain Callback Handler from here is deprecated. Please import with `from agentops.partners import LangchainCallbackHandler`\n"
     ]
    }
   ],
   "source": [
    "from agentops.langchain_callback_handler import (\n",
    "    LangchainCallbackHandler as AgentOpsLangchainCallbackHandler,\n",
    ")\n",
    "import agentops\n",
    "\n",
    "agentops.init(os.environ[\"AGENTOPS_API_KEY\"])\n",
    "agentops_handler = AgentOpsLangchainCallbackHandler(\n",
    "    api_key=os.environ[\"AGENTOPS_API_KEY\"], tags=[\"Plan and Execute\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first define the tools we want to use. For this example, we will use a powerfull search tool exa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tools = [TavilySearchResults(max_results=3)]\n",
    "for t in tools:\n",
    "    t.callbacks = [agentops_handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Execution Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following questions as best you can. You have access to the following tools:\n",
      "\n",
      "\u001b[33;1m\u001b[1;3m{tools}\u001b[0m\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [\u001b[33;1m\u001b[1;3m{tool_names}\u001b[0m]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n",
      "Thought:\u001b[33;1m\u001b[1;3m{agent_scratchpad}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_groq import ChatGroq\n",
    "# from langgraph.prebuilt import create_react_agent\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.pretty_print()\n",
    "\n",
    "llm = ChatGroq(callbacks=[agentops_handler])\n",
    "\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Who won the recent EURO CUP and COPA America final?',\n",
       " 'output': 'Italy won the recent EURO CUP and Argentina won the recent COPA America final.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Who won the recent EURO CUP and COPA America final?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the State\n",
    "\n",
    "Defining the state the track for this agent.\n",
    "\n",
    "First, we will need to track the current plan. Let's represent that as a list of strings.\n",
    "\n",
    "Nest we should track previously executed steps. Let's represent that as a list of tuples(these tuples will contain the step result)\n",
    "\n",
    "Finally, we need to have some state to represent the final response as well as the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, Tuple, TypedDict\n",
    "\n",
    "class PlanExecute(TypedDict):\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: Annotated[List[Tuple], operator.add]\n",
    "    response:str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning Step\n",
    "Creating the planning step. This will use function calling to create a plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"Plan to follow in future\"\"\"\n",
    "    \n",
    "    steps: List[str] = Field(\n",
    "        ...,\n",
    "        description = \"Steps to follow, in order\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"For the given objetive, come up with a simple step by step plan.\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Do not add any superfluous steps.\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - donot skip steps.\n",
    "Output your response in the following JSON format: \"steps\": [\"Step 1\",\"Step 2\",\"Step 3\"]\n",
    "\"\"\",\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "planner = planner_prompt | ChatGroq(callbacks=[agentops_handler]).with_structured_output(Plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plan(steps=['Identify the team that won the recent Euro Cup final', 'Find out the key players of the identified team'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            (\"user\", \"Who is the key player of the team winning the recent Euro Cup final?\")            \n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replan Step\n",
    "Now, lets create a step that re-does the plan based on the result of previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "class Response(BaseModel):\n",
    "    \"\"\"Response to user.\"\"\"\n",
    "    \n",
    "    response: str\n",
    "    \n",
    "class Act(BaseModel):\n",
    "    \"\"\"Action to perform.\"\"\"\n",
    "    \n",
    "    action: Union[Response, Plan] = Field(\n",
    "        description = \"Action to perform. If you find the answer or want to respond to user, use Response.\"\n",
    "        \"If you need to further use tools to get the answer, use Plan.\"\n",
    "    )\n",
    "    \n",
    "replanner_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"For the given objective, come up with a simple step by step plan.\n",
    "This plan should involve individual tasks, that if executed correctly will yield the correct answer. Donot add any super flous steps.\n",
    "The result of the final step should be the final answer. Make sure that each step has all the information needed - donot skip steps.\n",
    "\n",
    "Your objective was like this:\n",
    "{input}\n",
    "\n",
    "Your original plan was this:\n",
    "{plan}\n",
    "\n",
    "You have currently done the following steps:\n",
    "{past_steps}\n",
    "\n",
    "Update your plan accordingly. If no more steps are needed and you can return to the use, then respond with that.\n",
    "Otherwise, fill out the plan. Only add steps to the plan that still NEED to be done. Donot return previously done steps as part of the plan.\"\"\"\n",
    ")\n",
    "\n",
    "replanner = replanner_prompt | ChatGroq(callbacks=[agentops_handler]).with_structured_output(Act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (3820109515.py, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[63], line 55\u001b[1;36m\u001b[0m\n\u001b[1;33m    def should_end(state: PlanExecute) -> Literal[\"agent\", \"__end__\"]:\u001b[0m\n\u001b[1;37m                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "async def execute_step(state: PlanExecute):\n",
    "    plan = state[\"plan\"]\n",
    "    plan_str = \"\\n\".join(f\"{i+1}. {step}\" for i, step in enumerate(plan))\n",
    "    task = plan[0]\n",
    "    \n",
    "    task_formatted = f\"\"\"For the following plan: {plan_str}\\n\\nYou are tasked with executing step {1}, {task}.\"\"\"\n",
    "    agent_response = await agent_executor.ainvoke(\n",
    "        {\"input\": [(\"user\", task_formatted)]}\n",
    "    )\n",
    "    \n",
    "    if isinstance(agent_response, dict):\n",
    "        if \"response\" in agent_response:\n",
    "            content = agent_response[\"response\"]\n",
    "        elif \"output\" in agent_response:\n",
    "            content = agent_response[\"output\"]\n",
    "        else:\n",
    "            content = str(agent_response)  # Fallback to string representation\n",
    "    else:\n",
    "        content = str(agent_response)\n",
    "    \n",
    "    return {\n",
    "        \"past_steps\": [(task, content)]\n",
    "    }\n",
    "    \n",
    "\n",
    "async def plan_step(state: PlanExecute):\n",
    "    plan = await planner.ainvoke({\"messages\": [(\"user\", state[\"input\"])]})\n",
    "    return {\"plan\": plan.steps}\n",
    "\n",
    "\n",
    "async def replan_step(state: PlanExecute):\n",
    "    \n",
    "    output = await replanner.ainvoke(state)\n",
    "    if isinstance(output.action, Response):\n",
    "        return {\"response\": output.action.response}\n",
    "    else:\n",
    "        return {\"plan\": output.action.steps}\n",
    "    # except BadRequestError as e:\n",
    "    #     # If there's an error, try to extract the response from the error message\n",
    "    #     error_data = e.response.json()\n",
    "    #     failed_generation = error_data['error'].get('failed_generation', '')\n",
    "    #     if 'response' in failed_generation:\n",
    "    #         import json\n",
    "    #         try:\n",
    "    #             response_data = json.loads(failed_generation.replace('<tool-use>\\n', '').replace('</tool-use>', ''))\n",
    "    #             return {\"response\": response_data['tool_call']['parameters']['action']['response']['response']}\n",
    "    #         except json.JSONDecodeError:\n",
    "    #             pass\n",
    "    #     # If we can't extract a response, raise the error\n",
    "    #     raise\n",
    "    \n",
    "\n",
    "def should_end(state: PlanExecute) -> Literal[\"agent\", \"__end__\"]:\n",
    "    if \"response\" in state and state[\"response\"]:\n",
    "        return \"__end__\"\n",
    "    else:\n",
    "        return \"agent\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(PlanExecute)\n",
    "\n",
    "# Add the plan to the node\n",
    "workflow.add_node(\"planner\", plan_step)\n",
    "\n",
    "# Add the plan to execution step\n",
    "workflow.add_node(\"agent\", execute_step)\n",
    "\n",
    "# Add the replan node\n",
    "workflow.add_node(\"replan\", replan_step)\n",
    "\n",
    "workflow.set_entry_point(\"planner\")\n",
    "\n",
    "# From plan we go to agent\n",
    "workflow.add_edge(\"planner\", \"agent\")\n",
    "\n",
    "# From agent, we replan\n",
    "workflow.add_edge(\"agent\", \"replan\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"replan\",\n",
    "    # Next, we pass in the function that will dettermine which node is called next.\n",
    "    should_end,\n",
    ")\n",
    "\n",
    "# Finally, we compile it!\n",
    "# This compiles it into a LangChain Runnable,\n",
    "# meaning you can use it as you would ant other runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': ['Identify the current year', 'Find out the name of the political party in power in Pakistan during the identified year', 'Identify the leader of the identified political party', 'Confirm if the identified person is the current Prime Minister of Pakistan']}\n",
      "{'past_steps': [('Identify the current year', 'The current year is 2022.')]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ–‡ AgentOps: Error: maximum recursion depth exceeded\n",
      "WARNING:agentops:Error: maximum recursion depth exceeded\n",
      "WARNING:langchain_core.callbacks.manager:Error in LangchainCallbackHandler.on_llm_error callback: RecursionError('maximum recursion depth exceeded')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BadRequestError' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 35\u001b[0m, in \u001b[0;36mreplan_step\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m replanner\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output\u001b[38;5;241m.\u001b[39maction, Response):\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2913\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2912\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2913\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   2914\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5069\u001b[0m, in \u001b[0;36mRunnableBindingBase.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5063\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[0;32m   5064\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5065\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5066\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5067\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5068\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39mainvoke(\n\u001b[0;32m   5070\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   5071\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   5072\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   5073\u001b[0m     )\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:291\u001b[0m, in \u001b[0;36mBaseChatModel.ainvoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 291\u001b[0m llm_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[0;32m    292\u001b[0m     [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    293\u001b[0m     stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    294\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    295\u001b[0m     tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    296\u001b[0m     metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    297\u001b[0m     run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    298\u001b[0m     run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    300\u001b[0m )\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ChatGeneration, llm_result\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:713\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    712\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[0;32m    714\u001b[0m     prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    715\u001b[0m )\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:673\u001b[0m, in \u001b[0;36mBaseChatModel.agenerate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    662\u001b[0m             \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    663\u001b[0m                 run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    671\u001b[0m             ]\n\u001b[0;32m    672\u001b[0m         )\n\u001b[1;32m--> 673\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    674\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    675\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    677\u001b[0m ]\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:858\u001b[0m, in \u001b[0;36mBaseChatModel._agenerate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 858\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[0;32m    859\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    860\u001b[0m     )\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_groq\\chat_models.py:493\u001b[0m, in \u001b[0;36mChatGroq._agenerate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    492\u001b[0m }\n\u001b[1;32m--> 493\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client\u001b[38;5;241m.\u001b[39mcreate(messages\u001b[38;5;241m=\u001b[39mmessage_dicts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:578\u001b[0m, in \u001b[0;36mAsyncCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/openai/v1/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    580\u001b[0m     body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[0;32m    581\u001b[0m         {\n\u001b[0;32m    582\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    583\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    584\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    585\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    586\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    587\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    588\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    589\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    590\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    591\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    594\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    595\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    596\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    597\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    599\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    600\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    601\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    602\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    603\u001b[0m         },\n\u001b[0;32m    604\u001b[0m         completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    605\u001b[0m     ),\n\u001b[0;32m    606\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    607\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    608\u001b[0m     ),\n\u001b[0;32m    609\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    610\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    611\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mAsyncStream[ChatCompletionChunk],\n\u001b[0;32m    612\u001b[0m )\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\groq\\_base_client.py:1762\u001b[0m, in \u001b[0;36mAsyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1759\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1760\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1761\u001b[0m )\n\u001b[1;32m-> 1762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls)\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\groq\\_base_client.py:1478\u001b[0m, in \u001b[0;36mAsyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1471\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m     remaining_retries: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1477\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _AsyncStreamT:\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m   1479\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1480\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   1481\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m   1482\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1483\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m   1484\u001b[0m     )\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\groq\\_base_client.py:1569\u001b[0m, in \u001b[0;36mAsyncAPIClient._request\u001b[1;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[0;32m   1568\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1572\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1573\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1576\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1577\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Failed to call a function. Please adjust your prompt. See 'failed_generation' for more details.\", 'type': 'invalid_request_error', 'code': 'tool_use_failed', 'failed_generation': '<tool-use>\\n{\\n\\t\"tool_call\": {\\n\\t\\t\"id\": \"pending\",\\n\\t\\t\"type\": \"function\",\\n\\t\\t\"function\": {\\n\\t\\t\\t\"name\": \"Act\"\\n\\t\\t},\\n\\t\\t\"parameters\": {\\n\\t\\t\\t\"action\": {\\n\\t\\t\\t\\t\"response\": {\\n\\t\\t\\t\\t\\t\"response\": \"The current political party in power in Pakistan for the year 2022 is the Pakistan Tehreek-e-Insaf (PTI). The leader of the PTI is Imran Khan. Is Imran Khan the current Prime Minister of Pakistan?\"\\n\\t\\t\\t\\t}\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\\n</tool-use>'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m50\u001b[39m}\n\u001b[0;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWho is the current prime minister of Pakistan?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m app\u001b[38;5;241m.\u001b[39mastream(inputs, config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1195\u001b[0m, in \u001b[0;36mPregel.astream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m-> 1195\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTimeoutError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1349\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step, timeout_exc_cls)\u001b[0m\n\u001b[0;32m   1347\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1348\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1352\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1354\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langgraph\\pregel\\executor.py:123\u001b[0m, in \u001b[0;36mAsyncBackgroundExecutor.done\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdone\u001b[39m(\u001b[38;5;28mself\u001b[39m, task: asyncio\u001b[38;5;241m.\u001b[39mTask) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 123\u001b[0m         \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m GraphInterrupt:\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mpop(task)\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langgraph\\pregel\\retry.py:74\u001b[0m, in \u001b[0;36marun_with_retry\u001b[1;34m(task, retry_policy, stream)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, task\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2911\u001b[0m, in \u001b[0;36mRunnableSequence.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2907\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   2908\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2909\u001b[0m )\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2911\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2913\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32mt:\\Tyb\\Python\\RAG-Applications\\env\\Lib\\site-packages\\langgraph\\utils.py:124\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m--> 124\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), context\u001b[38;5;241m=\u001b[39mcontext\n\u001b[0;32m    126\u001b[0m     )\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[59], line 40\u001b[0m, in \u001b[0;36mreplan_step\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplan\u001b[39m\u001b[38;5;124m\"\u001b[39m: output\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39msteps}\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mBadRequestError\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# If there's an error, try to extract the response from the error message\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     error_data \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m     43\u001b[0m     failed_generation \u001b[38;5;241m=\u001b[39m error_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed_generation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BadRequestError' is not defined"
     ]
    }
   ],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"Who is the current prime minister of Pakistan?\"}\n",
    "\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\": \n",
    "            print(v)\n",
    "        else:\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plan': ['Identify the team that won the recent Euro Cup final', 'Find out the key players of the identified team']}\n",
      "{'past_steps': [('Identify the team that won the recent Euro Cup final', 'The winner of the recent Euro Cup final is Spain.')]}\n",
      "{'plan': ['Find out the key players of the Spanish national football team']}\n",
      "{'past_steps': [('Find out the key players of the Spanish national football team', 'The key players of the Spanish national football team include Lamine Yamal, Alvaro Morata, Nico Williams, Martin Zubimendi, Pedri, and Mikel Oyarzabal.')]}\n",
      "{'plan': ['Identify the key player of the Spanish national football team in the recent Euro Cup final']}\n",
      "{'past_steps': [('Identify the key player of the Spanish national football team in the recent Euro Cup final', \"The key player of the Spanish national football team in the recent Euro Cup final is Rodri, who was named the best player of Euro 2024 and made significant contributions to the team's performance throughout the tournament.\")]}\n",
      "{'plan': [\"Identify the key player of the Spanish national football team in the recent Euro Cup final based on the information that Rodri was named the best player of Euro 2024 and made significant contributions to the team's performance throughout the tournament.\"]}\n"
     ]
    }
   ],
   "source": [
    "config = {\"recursion_limit\": 50}\n",
    "inputs = {\"input\": \"Who is the key player of the team winning the recent Euro Cup final?\"}\n",
    "\n",
    "async for event in app.astream(inputs, config=config):\n",
    "    for k, v in event.items():\n",
    "        if k != \"__end__\": \n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
